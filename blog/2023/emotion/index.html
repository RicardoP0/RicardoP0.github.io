<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Emotion recognition using multimodal matchmap fusion and multi-task learning | Ricardo Pizarro</title> <meta name="author" content="Ricardo Pizarro"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="computer-vision, deep-learning, transformers, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ricardop0.github.io/blog/2023/emotion/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Emotion recognition using multimodal matchmap fusion and multi-task learning",
      "description": "",
      "published": "December 6, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ricardo </span>Pizarro</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">papers blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Emotion recognition using multimodal matchmap fusion and multi-task learning</h1> <p></p> </d-title><d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#abstract">Abstract</a></div> <div><a href="#introduction">Introduction</a></div> <div><a href="#the-experiment">The Experiment</a></div> <div><a href="#the-matchmap-fusion-method">The Matchmap Fusion Method</a></div> <div><a href="#results-in-a-nutshell">Results, in a Nutshell</a></div> <div><a href="#what-lies-ahead">What Lies Ahead</a></div> </nav> </d-contents> <h2 id="abstract---an-overview-of-our-research">Abstract - An Overview of Our Research</h2> <p>Recognizing emotions is a demanding task due to the immense intra-class and inter-class variability inherent to this issue. The way different individuals express a single emotion can vary, resulting in many potential interpretations. Also, some emotions resemble each other, thereby adding another layer of complexity.</p> <p>To tackle this problem, our research investigated the use of multimodal techniques, multi-task learning methods, and deep learning. A matchmap fusion method based on similarity between audio and video modalities was employed for solving the emotion classification issue. Our results depicted that utilizing the fusion method improved classification by 7%, in comparison to the baseline model.</p> <h2 id="introduction---why-emotion-recognition-matters">Introduction - Why Emotion Recognition Matters</h2> <p>Emotion recognition opens new frontiers for human-machine interactions, including everything from automatically generating robot responses depending on a user’s mood to estimating a patient’s mental state for prioritized health care.</p> <p>Our work focused on analyzing a multimodal estimation encompassing several input signals. In a nutshell, we tried to enhance the efficiency of the classifier or regressor by leveraging more than one input signal.</p> <h2 id="the-experiment---how-we-went-about-it">The Experiment - How We Went About It</h2> <p>In our experiment, we deployed an architecture that involved using log mel spectrograms as audio features and trained our convolutional network to capture more suitable features for the experiment. We used a Resnet3D architecture for estimating facial expressions from a video sequence.</p> <p>To merge the attributes from different modalities, we used matchmap, a fusion method based on the similarity between audio and video tensors.</p> <p>Two loss functions were used: first, a cross-entropy function for classification and second, a loss function based on matchmap tensor. Our experiments were carried out on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database. The IEMOCAP dataset comprises of five sessions that record ten speakers’ conversations. Each recorded conversation represents different emotional states and is used as training data. Two settings comprise the audio data: scripted and improvised speech. In the scripted setting, speakers are asked to read specific dialogues aloud, while the improvised setting includes spontaneous speech, closely resembling natural conversation. The collected data consists of 10,039 tagged utterances, each with an average duration of 4.5 seconds and a sample rate of 16 kHz.</p> <p><img src="/assets/img/grafico-paper-2.png" alt="architecture diagram"></p> <h2 id="the-matchmap-fusion-method---a-closer-look">The Matchmap Fusion Method - A Closer Look</h2> <p>Matchmap fusion is an advanced technique used in multimodal research. The main goal of this fusion is to represent the relationships between different modalities, such as audio and video. This involves creating a tensor that represents the similarity between different audio and video features.</p> <p>Let’s dive deep into how matchmap fusion works. After feeding video and audio data through their respective deep learning models, we obtain two sets of features - one from each modality. These tensor outputs are transposed and a dot product operation is performed between them, creating a new tensor, referred to as the matchmap tensor.</p> <p>The idea of a matchmap is to compute a localized similarity between a small region of audio and video modalities. It provides a way of capturing the synergistic relationship between different types of inputs and leveraging the complementary information they provide to solve the problem in hand.</p> <p>Let’s consider the problem of emotion recognition based on audio and video data. Here, the matchmap fusion can help to create representations that depict an emotion revealed in both audio and video sections. These learned relationships help enhance the distinction between different emotions and deal with the existing intra-class and inter-class variability, thereby improving the classification accuracy.</p> <p>Depending on the loss function used, various types of similarity functions can be derived from the matchmap tensor. This includes sum image sum audio (SISA), max image sum audio (MISA), and sum image max audio (SIMA), which can be used depending on the requirements of the specific task at hand.</p> <p>By using a matchmap fusion, we can develop more advanced and accurate classifiers, which can not only utilize the strengths of each modality but can also deal with the inherent difficulties associated with their combination. For example, in the case of emotion recognition, this fusion method is reported to have resulted in improvements in classification accuracy.</p> <h2 id="results-in-a-nutshell">Results, in a Nutshell</h2> <p>Our studies indicated that the use of multimodal techniques, deep learning and matchmap fusion considerably improved the process of emotion recognition. The results were also comparable with other leading research in this field.</p> <p>We demonstrated that a multimodal matchmap fusion method can give better results when one of the unimodal signals is underperforming. This proved that the algorithm allows the mixing of information from each modality, using all or little information that each mode provides improving inter-class results and reducing intra-class variability.</p> <h2 id="what-lies-ahead---future-scope">What Lies Ahead - Future Scope</h2> <p>Although we made significant progress, we believe that there is a lot of scope for improvement. We plan to further our research to perfect the emotion estimation results for video sequences from the IEMOCAP database. We are also exploring the integration of recurrent neural network techniques and LSTM networks for long-term dependencies inherent in this problem.</p> <p>Acknowledgment: We extend our gratitude to the Master of Computer Engineering and the Nucleus Research in Artificial Intelligence and Data Science from the Universidad Católica del Norte for their immense support.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Ricardo Pizarro. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 06, 2023. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>